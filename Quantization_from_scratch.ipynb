{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xFJWRL_hjivo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def w8_a16_forward(weight, input, scales, bias=None):\n",
        "\n",
        "    casted_weights = weight.to(input.dtype)\n",
        "    output = F.linear(input, casted_weights) * scales\n",
        "\n",
        "    if bias is not None:\n",
        "        output = output + bias\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class W8A16LinearLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features,\n",
        "                 bias=True, dtype=torch.float32):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"int8_weights\",\n",
        "            torch.randint(\n",
        "                -128, 127, (out_features, in_features), dtype=torch.int8\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.register_buffer(\"scales\",\n",
        "                             torch.randn((out_features), dtype=dtype))\n",
        "\n",
        "        if bias:\n",
        "            self.register_buffer(\"bias\",\n",
        "                                 torch.randn((1, out_features),\n",
        "                                             dtype=dtype))\n",
        "\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def quantize(self, weights):\n",
        "        w_fp32 = weights.clone().to(torch.float32)\n",
        "\n",
        "        scales = w_fp32.abs().max(dim=-1).values / 127\n",
        "        scales = scales.to(weights.dtype)\n",
        "\n",
        "        int8_weights = torch.round(weights\n",
        "                        /scales.unsqueeze(1)).to(torch.int8)\n",
        "\n",
        "        self.int8_weights = int8_weights\n",
        "        self.scales = scales\n",
        "\n",
        "    def forward(self, input):\n",
        "        return w8_a16_forward(self.int8_weights,\n",
        "                              input, self.scales, self.bias)\n",
        "\n",
        "def replace_linear_with_target_and_quantize(module,\n",
        "                               target_class, module_name_to_exclude):\n",
        "    for name, child in module.named_children():\n",
        "        if isinstance(child, nn.Linear) and not \\\n",
        "        any([x == name for x in module_name_to_exclude]):\n",
        "            old_bias = child.bias\n",
        "            old_weight = child.weight\n",
        "\n",
        "            new_module = target_class(child.in_features,\n",
        "                                      child.out_features,\n",
        "                                      old_bias is not None,\n",
        "                                      child.weight.dtype)\n",
        "            setattr(module, name, new_module)\n",
        "\n",
        "            getattr(module, name).quantize(old_weight)\n",
        "\n",
        "            if old_bias is not None:\n",
        "              getattr(module, name).bias = old_bias\n",
        "        else:\n",
        "            # Recursively call the function for nested modules\n",
        "            replace_linear_with_target_and_quantize(child,\n",
        "                     target_class, module_name_to_exclude)"
      ],
      "metadata": {
        "id": "nGQanxtWkJAa"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_linear_with_target_and_quantize(module,\n",
        "                               target_class, module_name_to_exclude):\n",
        "    for name, child in module.named_children():\n",
        "        if isinstance(child, nn.Linear) and not \\\n",
        "        any([x == name for x in module_name_to_exclude]):\n",
        "            old_bias = child.bias\n",
        "            old_weight = child.weight\n",
        "\n",
        "            new_module = target_class(child.in_features,\n",
        "                                      child.out_features,\n",
        "                                      old_bias is not None,\n",
        "                                      child.weight.dtype)\n",
        "            setattr(module, name, new_module)\n",
        "\n",
        "            getattr(module, name).quantize(old_weight)\n",
        "\n",
        "            if old_bias is not None:\n",
        "              getattr(module, name).bias = old_bias\n",
        "        else:\n",
        "            # Recursively call the function for nested modules\n",
        "            replace_linear_with_target_and_quantize(child,\n",
        "                     target_class, module_name_to_exclude)"
      ],
      "metadata": {
        "id": "tKbNjd-hkI9h"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "sZv0-LMqnhUH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "model_id = \"Salesforce/codegen-350M-mono\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             torch_dtype = torch.bfloat16,\n",
        "                                             )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"Salesforce/codegen-350M-mono\")"
      ],
      "metadata": {
        "id": "R9nGR3DLnlHt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipe(\"def hello_world():\", max_new_tokens=20, do_sample=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2toucodqAVq",
        "outputId": "c7f414b4-ed4a-4e5e-9a28-05bec6050d8e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'def hello_world():\\n    print(\"Hello World\")\\n\\nhello_world()\\n\\n# íŒŒ'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model before:\\n\\n\", model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rkoQCLeqiZc",
        "outputId": "990a31a1-88a3-408d-bd49-bde35d427863"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model before:\n",
            "\n",
            " CodeGenForCausalLM(\n",
            "  (transformer): CodeGenModel(\n",
            "    (wte): Embedding(51200, 1024)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-19): 20 x CodeGenBlock(\n",
            "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CodeGenAttention(\n",
            "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (qkv_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (mlp): CodeGenMLP(\n",
            "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "replace_linear_with_target_and_quantize(model,\n",
        "                                        W8A16LinearLayer, [\"lm_head\"])"
      ],
      "metadata": {
        "id": "HaPUIUu4rKyJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIRXnOgTrZyB",
        "outputId": "abbdda24-58e8-465a-cd29-6c154bf93e03"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CodeGenForCausalLM(\n",
              "  (transformer): CodeGenModel(\n",
              "    (wte): Embedding(51200, 1024)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-19): 20 x CodeGenBlock(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CodeGenAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (qkv_proj): W8A16LinearLayer()\n",
              "          (out_proj): W8A16LinearLayer()\n",
              "        )\n",
              "        (mlp): CodeGenMLP(\n",
              "          (fc_in): W8A16LinearLayer()\n",
              "          (fc_out): W8A16LinearLayer()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-8IV_hfBrehA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yb2BMBzmsbAU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}